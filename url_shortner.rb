# Review/attempt the bonuses

# Step 1: Scope
# What are we building? What features do we need?
#
# Is this a full web application with an interface?
# Let's assume that no, we just have an API.
#
# Since it's an API do we need auth or user accounts or dev
# keys? No, let's make it open access to start.
#
# Can people modify or delete links?
# Not for now.
#
# If people can't delete links, do they persist forever or
# do we remove old ones?
# It's worth considering the policies we have for removing
# old links.
# 1. Expiration date (using a created by timestamp) AKA 1 year
# 2. Links that haven't been visited in sometime AKA 1 year
# It is dangerous to remove links, because we make assumptions
# about when they will be used. Disc is cheap, let's let them exist
# forever.
#
# Do people choose their own shortlinks or are they autogenerated? I
# think this is something people would want, haven't some random hash
# is a bad ux.
#
# Do we need analytics so people can see how many people are visiting the
# link? Interesting. Let's leave that out for now.
#
# We need to map some generated url to a given url
# Then route a request to that address to the given url
#
# Step 2: Design Goals
# What are we optimizing for? It isn't space. We are storing links
# forever. The redirect should happen as fast as possible. So,
# the generation of the link and the lookup of the link should
# be as fast as possible. We need to index the URLs for lookup speed.
# Not to mention checking if names already exist.
#
# We want to store a lot of links. Maybe billions since they last forever
# We want to have as short of a link as possible while still having a
# valid lookup. The shortest is just to hash the id of each URL if we
# store it in a DB
# We want fast lookup, following the link should be fast
# We need to resilient to load spikes. We need to handle many users following
# many links all at once.
#
# In order of importance:
# 1. Most important is storing the links
# 2. Having the link WORK (due to load spikes)
# 3. Fast look up
# 4. Shortest link possible
#
#
# Step 3: Building the data model
# The models we need at this point are the link
# ShortLink
# - id
# - destination for redirect
# - slug for lookup
# - created_at for expiration?
# other meta data?
#
# Step 4: Endpoints/Views needed
# GET shorty.com/api/v1/shortlinks/new
#
# POST shorty.com/api/v1/shortlink(:slug, :destination)
# {
#   "slug":"aks39",
#   "destination":"example.com"
# }
#
# GET shorty.com/(:slug)
#
# Step 4: Core feature Algo
# Will want to handle the case where user tries to create
# a url that exists and come back with some suggestions
#
# What characters are allowed in a URL? Will need to check the
# spec here. Think it's only alpha numeric.
#
# 6 billion * 10 * 365 = ~25 trillion
# Gives us c^n -> 30 * log(n) == log(25,000,000,00)
#
# where c is the character set, which is?
# How many distinct short links do we want to accomodate?
# How long do they need to be to accomodate that many distinct
# possibilities
#
# Note: Stop and identify the steps
#
# Only characters allowed in URLs, I'm sure this is defined in
# an RFC somewhere
# Easy for the user to type in
#
# To generate the slugs we can have an incrementing hash function
# otherwise handling collisions will be a pain in the butt
#
# I need some way to hash ids, but then again, it is possible
# that some created ShortLink could take up the spot the id would
# map to. Each shortlink slug needs to be unique.
#
# How do we ensure uniqueness? A one way hash of ids is one way.
# Or we can check a value against the DB and see if a hashed value
# exists. If it doesn't exist, we are fine, otherwise search for
# the closest non occupied bucket.
#
# We can continuously reroll a random slug. But this could have
# some really undesirable behavior. And take up to O(n) time.
# We want a strategy to give unclaimed slugs.
